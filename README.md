# Top-ùëò Representative Spatial Objects using Spatial Diversified Proportionality

This repository contains the code to reproduce the experiments for the paper **"Spatial Diversified proportionality (Spatial Fairness)"**.

The project implements and evaluates a novel retrieval paradigm that selects a subset of `k` places from an initial set `S` that are not only relevant but also **spatially proportional** and **diverse**. This approach is formalized under the concept of **spatial fairness**.

---

## Core Algorithms

The main algorithms from the paper are implemented in the `src/` directory. Below are pointers to the key functions:

*   **Baseline IAdU Algorithm**: The exact, non-grid-based iterative algorithm is implemented as `baseline_iadu_algorithm` in `src/baseline_iadu.py`. This function performs the iterative selection on the full dataset `S`. The corresponding exact `pSS` pre-computation is handled by `base_precompute` in the same file.

*   **Virtual Grid pSS Approximation**: The approximation of `pSS` scores using a virtual grid is implemented in `virtual_grid_based_algorithm` within `src/grid_iadu.py`. This function calculates approximate spatial proportionality scores based on cell-level aggregations.

*   **Grid-based IAdU Algorithm**: The efficient grid-based version of the IAdU algorithm is implemented as `grid_based_iadu_algorithm` in `src/grid_iadu.py`. It uses a heap-per-cell strategy to prune the search space and accelerate the selection process.

*   **Pruning of S and Retrieval of R Algrithms**:  The hybrid approaches that combine biased sampling with the IAdU and Grid Based IAdU algorithms are implemented in `src/hybrid_sampling.py`. The functions `hybrid` and `hybrid_on_grid` orchestrate the sampling, execution of the appropriate IAdU variant on the sample, and the final scoring. Biased Sampling (Retrieval of R) algorithms are located in `src/biased_sampling.py`.

---

## Running the Experiments

To run the experiments, please follow these steps:

### 1. **Configuration**

The main configuration for all experiments is located in the `src/config.py` file. Before running any scripts, you should set the desired parameters in this file.

Here's an overview of the key parameters, explained using the terminology from the paper:



* **`NUM_CELLS`**: A list of integers representing the total number of cells (`|G|`) for the **Virtual Grid Based Algorithm**. This parameter controls the granularity of the grid used to approximate the spatial proportionality scores (`pSS(pi)`). A higher value results in a finer grid and can improve approximation quality at the cost of computation time.

* **`COMBO`**: A list of tuples, where each tuple `(K, k)` defines an experimental run.
    * `K`: The total number of relevant places in the initial set `S`.
    * `k`: The number of places to be selected for the final result set `R`.
    * **Example**: `COMBO = [(1000, 20), (5000, 50)]` will run experiments for an initial set of 1000 places to select 20, and for an initial set of 5000 places to select 50.

* **`GAMMAS`**: A list of float values for the weight parameter `w`. This weight is used in the spatial proportionality score, `pS(pi) = pSS(pi) - w * pSR(pi)`, to control the trade-off between favoring places in dense areas (`pSS(pi)`) and ensuring diversity among the selected places in `R` (`pSR(pi)`).

---

### 2. **Datasets**

The experiments use queries derived from the datasets **DBpedia** and **YAGO2**, as described in the paper. The pre-processed queries are expected to be in the `datasets/` directory.

## Generating Dataset Files (.pkl)

The `.pkl` dataset files, which represent the pre-processed queries, are generated by the `src/dbpedia_query_generator.py` and `src/yago2_query_generator.py` scripts. These scripts take raw data (e.g., `pid.txt` and popular region definitions) and produce nested subsets of places for various `K` values. The output `.pkl` files are typically saved into `dbpedia_output/` and `yago_square/` directories, respectively, which should then be moved or linked to the `datasets/` directory for the main experiment scripts to find them. For example, to generate the DBpedia query files, run `python src/dbpedia_query_generator.py`.

Please ensure the `.pkl` and `.npy` dataset files are correctly named and placed in the `datasets/` directory so the `dataset_store.py` script can load them properly.

**DBpedia** : https://www.dbpedia.org

**YAGO2**   : https://yago-knowledge.org/downloads/yago-2

---

### 3. **Dependencies**

This project requires Python 3. You can install the necessary libraries using pip. Based on the project's scope, you will likely need the following:

```bash
pip install numpy pandas scikit-learn matplotlib
```

### 4. **Running the Experiments**

The experiment scripts are located in the `src/exp/` directory. To run an experiment, you can execute the desired Python script from the `src/` directory.

For example, to run the `hardcore_exp.py` experiment, you would run the following command from the root of the project:

```bash
python src/exp/hardcore_exp.py
```

This will run the experiment with the parameters you have defined in `src/config.py`. The script will likely iterate through all the configured datasets, grid sizes, and other parameters, running the necessary algorithms and saving the results.

---

### 5. **Results**

The output of the experiments, such as logs, plots, or raw data, will be saved to a directory (e.g., `results/` or `output/`). You may need to create this directory if the scripts don't do it automatically.
